{"cells":[{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.540704Z","iopub.status.busy":"2024-10-28T21:04:27.540137Z","iopub.status.idle":"2024-10-28T21:04:27.547038Z","shell.execute_reply":"2024-10-28T21:04:27.545626Z","shell.execute_reply.started":"2024-10-28T21:04:27.540662Z"},"trusted":true},"outputs":[],"source":["#importing libraries\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import urllib.request\n","import re\n","from collections import Counter"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.549439Z","iopub.status.busy":"2024-10-28T21:04:27.548986Z","iopub.status.idle":"2024-10-28T21:04:27.560491Z","shell.execute_reply":"2024-10-28T21:04:27.558650Z","shell.execute_reply.started":"2024-10-28T21:04:27.549391Z"},"trusted":true},"outputs":[],"source":["#setting device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.563208Z","iopub.status.busy":"2024-10-28T21:04:27.562730Z","iopub.status.idle":"2024-10-28T21:04:27.782055Z","shell.execute_reply":"2024-10-28T21:04:27.780850Z","shell.execute_reply.started":"2024-10-28T21:04:27.563162Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{ The Project Gutenberg EBook of Emma, by Jane Austen\\ \\ This eBook is for the use of anyone anywhere at no cost and with\\ almost no restrictions whatsoever. You may copy it, give it away or\\ re-use it under the terms of the Project Gutenberg License included\\ with this eBook or online at www.gutenberg.org\\ \\ \\ Title: Emma\\ \\ Author: Jane Austen\\ \\ Release Date: August, 1994 [Etext #158]\\ Posting Date: January 21, 2010\\ Last Updated: October 17, 2016\\ \\ Language: English\\ \\ Character set encodin\n"]}],"source":["import re\n","\n","#path of the RTF file\n","file_path = \"Emma_by_Jane_Austen.rtf\"\n","\n","#reading the RTF file\n","with open(file_path, 'r', encoding='utf-8') as file:\n","    rtf_content = file.read()\n","\n","#function to clean RTF file\n","def clean_rtf(rtf):\n","    # Remove RTF formatting\n","    # This regex removes everything that isn't plain text\n","    cleaned_text = re.sub(r'{\\\\.*?}', '', rtf)  # Remove RTF groups\n","    cleaned_text = re.sub(r'\\\\[a-z]+\\d* ?', '', cleaned_text)  # Remove RTF commands\n","    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Remove extra spaces\n","    return cleaned_text.strip()\n","\n","#cleaning the RTF text\n","plain_text = clean_rtf(rtf_content)\n","\n","#displaying the first 500 characters of the plain text\n","print(plain_text[:500])\n"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.783782Z","iopub.status.busy":"2024-10-28T21:04:27.783451Z","iopub.status.idle":"2024-10-28T21:04:27.826563Z","shell.execute_reply":"2024-10-28T21:04:27.825301Z","shell.execute_reply.started":"2024-10-28T21:04:27.783747Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" the project gutenberg ebook of emma by jane austen  this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. you may copy it give it away or reuse it under the terms of the project gutenberg license included with this ebook or online at www.gutenberg.org   title emma  author jane austen  release date august 1994 etext 158 posting date january 21 2010 las\n"]}],"source":["#converting text to lowercase\n","plain_text = plain_text.lower()\n","\n","#removing unwanted characters\n","cleaned_text = re.sub('[^a-zA-Z0-9 .]', '', plain_text)\n","\n","#splitting into words\n","words = cleaned_text.split()\n","\n","print(cleaned_text[:400])"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.829482Z","iopub.status.busy":"2024-10-28T21:04:27.829077Z","iopub.status.idle":"2024-10-28T21:04:27.863434Z","shell.execute_reply":"2024-10-28T21:04:27.861947Z","shell.execute_reply.started":"2024-10-28T21:04:27.829437Z"},"trusted":true},"outputs":[],"source":["#creating vocab of unique words\n","words_vocab = sorted(set(words))\n","stoi = {s: i for i, s in enumerate(words_vocab)}\n","itos = {i: s for i, s in enumerate(words_vocab)}"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.865638Z","iopub.status.busy":"2024-10-28T21:04:27.865180Z","iopub.status.idle":"2024-10-28T21:04:27.875165Z","shell.execute_reply":"2024-10-28T21:04:27.873362Z","shell.execute_reply.started":"2024-10-28T21:04:27.865587Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["160442\n"]}],"source":["print(len(words))"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.877058Z","iopub.status.busy":"2024-10-28T21:04:27.876667Z","iopub.status.idle":"2024-10-28T21:04:27.887174Z","shell.execute_reply":"2024-10-28T21:04:27.886052Z","shell.execute_reply.started":"2024-10-28T21:04:27.877020Z"},"trusted":true},"outputs":[],"source":["#function to create input-output pairs\n","def create_dataset(words, block_size):\n","    X, Y = [], []\n","    for i in range(len(words) - block_size):\n","        context = [stoi[words[j]] for j in range(i, i + block_size)]\n","        next_word = stoi[words[i + block_size]]\n","        X.append(context)\n","        Y.append(next_word)\n","    return torch.tensor(X).to(device), torch.tensor(Y).to(device)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.889306Z","iopub.status.busy":"2024-10-28T21:04:27.888734Z","iopub.status.idle":"2024-10-28T21:04:27.903543Z","shell.execute_reply":"2024-10-28T21:04:27.902342Z","shell.execute_reply.started":"2024-10-28T21:04:27.889242Z"},"trusted":true},"outputs":[],"source":["class NextWordMLP(nn.Module):\n","    def __init__(self, block_size, vocab_size, emb_dim, hidden_size, activation):\n","        super().__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.lin1 = nn.Linear(block_size * emb_dim, hidden_size)\n","        self.activation = activation\n","        self.lin2 = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, x):\n","        x = self.emb(x)\n","        x = x.view(x.shape[0], -1)\n","        x = self.activation(self.lin1(x))\n","        x = self.lin2(x)\n","        return x"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.906424Z","iopub.status.busy":"2024-10-28T21:04:27.905880Z","iopub.status.idle":"2024-10-28T21:04:27.915466Z","shell.execute_reply":"2024-10-28T21:04:27.914078Z","shell.execute_reply.started":"2024-10-28T21:04:27.906372Z"},"trusted":true},"outputs":[],"source":["embedding_sizes = [64, 128]\n","context_lengths = [5, 10, 15]\n","activations = [F.relu, torch.tanh]\n","epochs = 500"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.918127Z","iopub.status.busy":"2024-10-28T21:04:27.917595Z","iopub.status.idle":"2024-10-28T21:04:27.930137Z","shell.execute_reply":"2024-10-28T21:04:27.928828Z","shell.execute_reply.started":"2024-10-28T21:04:27.918074Z"},"trusted":true},"outputs":[],"source":["def train_model(embedding_size, block_size, activation_fn):\n","    hidden_size = 512\n","    model = NextWordMLP(block_size, len(stoi), embedding_size, hidden_size, activation_fn).to(device)\n","    loss_fn = nn.CrossEntropyLoss()\n","    opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n","    X, Y = create_dataset(words, block_size)\n","\n","    \n","    batch_size = 512\n","    losses = []\n","    for epoch in range(epochs):\n","        epoch_loss = 0\n","        for i in range(0, len(X), batch_size):\n","            x_batch = X[i:i + batch_size]\n","            y_batch = Y[i:i + batch_size]\n","            y_pred = model(x_batch)\n","            loss = loss_fn(y_pred, y_batch)\n","            loss.backward()\n","            opt.step()\n","            opt.zero_grad()\n","            epoch_loss += loss.item()\n","        \n","        losses.append(epoch_loss / (len(X) // batch_size))\n","        if epoch % 1 == 0:\n","            print(f\"Epoch {epoch}, Loss: {losses[-1]:.4f}\")\n","\n","        #early stopping in case loss plateaus\n","        if len(losses) > 10 and abs(losses[-1] - losses[-10]) < 0.001:\n","            print(\"Early stopping\")\n","            break\n","\n","    return model"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.933415Z","iopub.status.busy":"2024-10-28T21:04:27.932983Z","iopub.status.idle":"2024-10-28T21:04:27.947322Z","shell.execute_reply":"2024-10-28T21:04:27.945947Z","shell.execute_reply.started":"2024-10-28T21:04:27.933373Z"},"trusted":true},"outputs":[],"source":["def generate_text(model, itos, stoi, block_size, max_length=50):\n","    context = [0] * block_size\n","    generated_words = []\n","    for _ in range(max_length):\n","        x = torch.tensor(context).view(1, -1).to(device)\n","        y_pred = model(x)\n","        ix = torch.distributions.categorical.Categorical(logits=y_pred).sample().item()\n","        word = itos[ix]\n","        generated_words.append(word)\n","        context = context[1:] + [ix]\n","    return ' '.join(generated_words)"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["def save_model(model, embedding_size, block_size, activation_fn_name):\n","    model_filename = f\"saved_models/model_emb{embedding_size}_ctx{block_size}_act{activation_fn_name}.pt\"\n","    torch.save(model.state_dict(), model_filename)\n","    print(f\"Model saved as {model_filename}\")"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training with Embedding Size: 64, Context Length: 5, Activation: relu\n","Epoch 0, Loss: 6.5846\n","Epoch 1, Loss: 5.5008\n","Epoch 2, Loss: 4.7264\n","Epoch 3, Loss: 3.7315\n","Epoch 4, Loss: 3.1709\n","Epoch 5, Loss: 2.8659\n","Epoch 6, Loss: 2.6344\n","Epoch 7, Loss: 2.4448\n","Epoch 8, Loss: 2.2835\n","Epoch 9, Loss: 2.1424\n","Epoch 10, Loss: 2.0172\n","Epoch 11, Loss: 1.9042\n","Epoch 12, Loss: 1.8009\n","Epoch 13, Loss: 1.7061\n","Epoch 14, Loss: 1.6177\n","Epoch 15, Loss: 1.5356\n","Epoch 16, Loss: 1.4581\n","Epoch 17, Loss: 1.3852\n","Epoch 18, Loss: 1.3161\n","Epoch 19, Loss: 1.2507\n","Epoch 20, Loss: 1.1882\n","Epoch 21, Loss: 1.1287\n","Epoch 22, Loss: 1.0718\n","Epoch 23, Loss: 1.0170\n","Epoch 24, Loss: 0.9647\n","Epoch 25, Loss: 0.9147\n","Epoch 26, Loss: 0.8663\n","Epoch 27, Loss: 0.8202\n","Epoch 28, Loss: 0.7759\n","Epoch 29, Loss: 0.7332\n","Epoch 30, Loss: 0.6920\n","Epoch 31, Loss: 0.6530\n","Epoch 32, Loss: 0.6153\n","Epoch 33, Loss: 0.5790\n","Epoch 34, Loss: 0.5443\n","Epoch 35, Loss: 0.5114\n","Epoch 36, Loss: 0.4793\n","Epoch 37, Loss: 0.4489\n","Epoch 38, Loss: 0.4198\n","Epoch 39, Loss: 0.3921\n","Epoch 40, Loss: 0.3652\n","Epoch 41, Loss: 0.3402\n","Epoch 42, Loss: 0.3161\n","Epoch 43, Loss: 0.2933\n","Epoch 44, Loss: 0.2718\n","Epoch 45, Loss: 0.2513\n","Epoch 46, Loss: 0.2319\n","Epoch 47, Loss: 0.2145\n","Epoch 48, Loss: 0.1973\n","Epoch 49, Loss: 0.1819\n","Epoch 50, Loss: 0.1681\n","Epoch 51, Loss: 0.1552\n","Epoch 52, Loss: 0.1437\n","Epoch 53, Loss: 0.1338\n","Epoch 54, Loss: 0.1247\n","Epoch 55, Loss: 0.1172\n","Epoch 56, Loss: 0.1109\n","Epoch 57, Loss: 0.1064\n","Epoch 58, Loss: 0.1019\n","Epoch 59, Loss: 0.0983\n","Epoch 60, Loss: 0.0950\n","Epoch 61, Loss: 0.0903\n","Epoch 62, Loss: 0.0868\n","Epoch 63, Loss: 0.0835\n","Epoch 64, Loss: 0.0813\n","Epoch 65, Loss: 0.0793\n","Epoch 66, Loss: 0.0783\n","Epoch 67, Loss: 0.0769\n","Epoch 68, Loss: 0.0759\n","Epoch 69, Loss: 0.0746\n","Epoch 70, Loss: 0.0731\n","Epoch 71, Loss: 0.0719\n","Epoch 72, Loss: 0.0715\n","Epoch 73, Loss: 0.0700\n","Epoch 74, Loss: 0.0694\n","Epoch 75, Loss: 0.0690\n","Epoch 76, Loss: 0.0680\n","Epoch 77, Loss: 0.0672\n","Epoch 78, Loss: 0.0665\n","Epoch 79, Loss: 0.0658\n","Epoch 80, Loss: 0.0652\n","Epoch 81, Loss: 0.0648\n","Epoch 82, Loss: 0.0643\n","Epoch 83, Loss: 0.0634\n","Epoch 84, Loss: 0.0627\n","Epoch 85, Loss: 0.0620\n","Epoch 86, Loss: 0.0610\n","Epoch 87, Loss: 0.0604\n","Epoch 88, Loss: 0.0598\n","Epoch 89, Loss: 0.0592\n","Epoch 90, Loss: 0.0586\n","Epoch 91, Loss: 0.0578\n","Epoch 92, Loss: 0.0572\n","Epoch 93, Loss: 0.0567\n","Epoch 94, Loss: 0.0559\n","Epoch 95, Loss: 0.0553\n","Epoch 96, Loss: 0.0548\n","Epoch 97, Loss: 0.0544\n","Epoch 98, Loss: 0.0533\n","Epoch 99, Loss: 0.0530\n","Epoch 100, Loss: 0.0522\n","Epoch 101, Loss: 0.0518\n","Epoch 102, Loss: 0.0513\n","Epoch 103, Loss: 0.0509\n","Epoch 104, Loss: 0.0501\n","Epoch 105, Loss: 0.0500\n","Epoch 106, Loss: 0.0491\n","Epoch 107, Loss: 0.0491\n","Epoch 108, Loss: 0.0482\n","Epoch 109, Loss: 0.0478\n","Epoch 110, Loss: 0.0470\n","Epoch 111, Loss: 0.0469\n","Epoch 112, Loss: 0.0460\n","Epoch 113, Loss: 0.0458\n","Epoch 114, Loss: 0.0452\n","Epoch 115, Loss: 0.0450\n","Epoch 116, Loss: 0.0443\n","Epoch 117, Loss: 0.0444\n","Epoch 118, Loss: 0.0435\n","Epoch 119, Loss: 0.0433\n","Epoch 120, Loss: 0.0428\n","Epoch 121, Loss: 0.0426\n","Epoch 122, Loss: 0.0419\n","Epoch 123, Loss: 0.0420\n","Epoch 124, Loss: 0.0412\n","Epoch 125, Loss: 0.0410\n","Epoch 126, Loss: 0.0404\n","Epoch 127, Loss: 0.0404\n","Epoch 128, Loss: 0.0401\n","Epoch 129, Loss: 0.0400\n","Epoch 130, Loss: 0.0390\n","Epoch 131, Loss: 0.0388\n","Epoch 132, Loss: 0.0384\n","Epoch 133, Loss: 0.0383\n","Epoch 134, Loss: 0.0376\n","Epoch 135, Loss: 0.0378\n","Epoch 136, Loss: 0.0372\n","Epoch 137, Loss: 0.0370\n","Epoch 138, Loss: 0.0365\n","Epoch 139, Loss: 0.0364\n","Epoch 140, Loss: 0.0357\n","Epoch 141, Loss: 0.0357\n","Epoch 142, Loss: 0.0352\n","Epoch 143, Loss: 0.0353\n","Epoch 144, Loss: 0.0347\n","Epoch 145, Loss: 0.0346\n","Epoch 146, Loss: 0.0341\n","Epoch 147, Loss: 0.0341\n","Epoch 148, Loss: 0.0336\n","Epoch 149, Loss: 0.0335\n","Epoch 150, Loss: 0.0330\n","Epoch 151, Loss: 0.0330\n","Epoch 152, Loss: 0.0325\n","Epoch 153, Loss: 0.0325\n","Epoch 154, Loss: 0.0320\n","Epoch 155, Loss: 0.0319\n","Epoch 156, Loss: 0.0315\n","Epoch 157, Loss: 0.0314\n","Epoch 158, Loss: 0.0310\n","Epoch 159, Loss: 0.0310\n","Epoch 160, Loss: 0.0305\n","Epoch 161, Loss: 0.0304\n","Epoch 162, Loss: 0.0301\n","Epoch 163, Loss: 0.0304\n","Epoch 164, Loss: 0.0527\n","Epoch 165, Loss: 0.1473\n","Epoch 166, Loss: 0.0425\n","Epoch 167, Loss: 0.0232\n","Epoch 168, Loss: 0.0217\n","Epoch 169, Loss: 0.0220\n","Epoch 170, Loss: 0.0225\n","Epoch 171, Loss: 0.0231\n","Epoch 172, Loss: 0.0237\n","Epoch 173, Loss: 0.0244\n","Epoch 174, Loss: 0.0251\n","Epoch 175, Loss: 0.0258\n","Epoch 176, Loss: 0.0265\n","Epoch 177, Loss: 0.0269\n","Epoch 178, Loss: 0.0273\n","Epoch 179, Loss: 0.0277\n","Epoch 180, Loss: 0.0278\n","Epoch 181, Loss: 0.0280\n","Epoch 182, Loss: 0.0283\n","Epoch 183, Loss: 0.0283\n","Epoch 184, Loss: 0.0283\n","Epoch 185, Loss: 0.0282\n","Epoch 186, Loss: 0.0282\n","Epoch 187, Loss: 0.0280\n","Early stopping\n","Model saved as saved_models/model_emb64_ctx5_actrelu.pt\n","Generated Text:\n","to your of miss fairfax92s situation in life. i will move a little farther off.94 93i certainly do forget to think of her94 said emma 93as having ever been any thing but my friend and my dearest friend.94 he looked as if he felt to me me. i am still not the least at randalls. up could want of miss fairfax92s greater words for what the knowing he was though but at his daughter92s being cannot be take it so as to what had been most happy young superior an admiration to all so long a party from her well\n"]}],"source":["embedding_size = embedding_sizes[0]\n","block_size = context_lengths[0]\n","activation_fn = activations[0]\n","print(f\"\\nTraining with Embedding Size: {embedding_size}, Context Length: {block_size}, Activation: {activation_fn.__name__}\")\n","model1 = train_model(embedding_size, block_size, activation_fn)\n","save_model(model1, embedding_size, block_size, activation_fn.__name__)\n","print(\"Generated Text:\")\n","print(generate_text(model1, itos, stoi, block_size, max_length=100))"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training with Embedding Size: 64, Context Length: 5, Activation: tanh\n","Epoch 0, Loss: 6.8163\n","Epoch 1, Loss: 5.5102\n","Epoch 2, Loss: 4.8018\n","Epoch 3, Loss: 4.1668\n","Epoch 4, Loss: 3.7250\n","Epoch 5, Loss: 3.4147\n","Epoch 6, Loss: 3.1718\n","Epoch 7, Loss: 2.9695\n","Epoch 8, Loss: 2.7932\n","Epoch 9, Loss: 2.6370\n","Epoch 10, Loss: 2.4941\n","Epoch 11, Loss: 2.3626\n","Epoch 12, Loss: 2.2395\n","Epoch 13, Loss: 2.1240\n","Epoch 14, Loss: 2.0145\n","Epoch 15, Loss: 1.9110\n","Epoch 16, Loss: 1.8115\n","Epoch 17, Loss: 1.7170\n","Epoch 18, Loss: 1.6260\n","Epoch 19, Loss: 1.5390\n","Epoch 20, Loss: 1.4551\n","Epoch 21, Loss: 1.3752\n","Epoch 22, Loss: 1.2977\n","Epoch 23, Loss: 1.2238\n","Epoch 24, Loss: 1.1525\n","Epoch 25, Loss: 1.0849\n","Epoch 26, Loss: 1.0195\n","Epoch 27, Loss: 0.9575\n","Epoch 28, Loss: 0.8982\n","Epoch 29, Loss: 0.8414\n","Epoch 30, Loss: 0.7873\n","Epoch 31, Loss: 0.7365\n","Epoch 32, Loss: 0.6873\n","Epoch 33, Loss: 0.6410\n","Epoch 34, Loss: 0.5972\n","Epoch 35, Loss: 0.5557\n","Epoch 36, Loss: 0.5164\n","Epoch 37, Loss: 0.4789\n","Epoch 38, Loss: 0.4437\n","Epoch 39, Loss: 0.4105\n","Epoch 40, Loss: 0.3794\n","Epoch 41, Loss: 0.3503\n","Epoch 42, Loss: 0.3226\n","Epoch 43, Loss: 0.2973\n","Epoch 44, Loss: 0.2733\n","Epoch 45, Loss: 0.2512\n","Epoch 46, Loss: 0.2305\n","Epoch 47, Loss: 0.2118\n","Epoch 48, Loss: 0.1942\n","Epoch 49, Loss: 0.1787\n","Epoch 50, Loss: 0.1640\n","Epoch 51, Loss: 0.1515\n","Epoch 52, Loss: 0.1393\n","Epoch 53, Loss: 0.1294\n","Epoch 54, Loss: 0.1200\n","Epoch 55, Loss: 0.1122\n","Epoch 56, Loss: 0.1050\n","Epoch 57, Loss: 0.0996\n","Epoch 58, Loss: 0.0941\n","Epoch 59, Loss: 0.0900\n","Epoch 60, Loss: 0.0860\n","Epoch 61, Loss: 0.0828\n","Epoch 62, Loss: 0.0795\n","Epoch 63, Loss: 0.0773\n","Epoch 64, Loss: 0.0746\n","Epoch 65, Loss: 0.0725\n","Epoch 66, Loss: 0.0704\n","Epoch 67, Loss: 0.0687\n","Epoch 68, Loss: 0.0668\n","Epoch 69, Loss: 0.0658\n","Epoch 70, Loss: 0.0637\n","Epoch 71, Loss: 0.0629\n","Epoch 72, Loss: 0.0618\n","Epoch 73, Loss: 0.0609\n","Epoch 74, Loss: 0.0590\n","Epoch 75, Loss: 0.0587\n","Epoch 76, Loss: 0.0573\n","Epoch 77, Loss: 0.0567\n","Epoch 78, Loss: 0.0555\n","Epoch 79, Loss: 0.0549\n","Epoch 80, Loss: 0.0540\n","Epoch 81, Loss: 0.0538\n","Epoch 82, Loss: 0.0527\n","Epoch 83, Loss: 0.0522\n","Epoch 84, Loss: 0.0519\n","Epoch 85, Loss: 0.0512\n","Epoch 86, Loss: 0.0503\n","Epoch 87, Loss: 0.0498\n","Epoch 88, Loss: 0.0492\n","Epoch 89, Loss: 0.0488\n","Epoch 90, Loss: 0.0485\n","Epoch 91, Loss: 0.0479\n","Epoch 92, Loss: 0.0474\n","Epoch 93, Loss: 0.0475\n","Epoch 94, Loss: 0.0466\n","Epoch 95, Loss: 0.0464\n","Epoch 96, Loss: 0.0463\n","Epoch 97, Loss: 0.0458\n","Epoch 98, Loss: 0.0451\n","Epoch 99, Loss: 0.0453\n","Epoch 100, Loss: 0.0445\n","Epoch 101, Loss: 0.0443\n","Epoch 102, Loss: 0.0442\n","Epoch 103, Loss: 0.0439\n","Epoch 104, Loss: 0.0434\n","Epoch 105, Loss: 0.0433\n","Epoch 106, Loss: 0.0430\n","Epoch 107, Loss: 0.0428\n","Epoch 108, Loss: 0.0426\n","Epoch 109, Loss: 0.0425\n","Epoch 110, Loss: 0.0419\n","Epoch 111, Loss: 0.0421\n","Epoch 112, Loss: 0.0416\n","Epoch 113, Loss: 0.0414\n","Epoch 114, Loss: 0.0413\n","Epoch 115, Loss: 0.0411\n","Epoch 116, Loss: 0.0407\n","Epoch 117, Loss: 0.0409\n","Epoch 118, Loss: 0.0405\n","Epoch 119, Loss: 0.0403\n","Epoch 120, Loss: 0.0402\n","Epoch 121, Loss: 0.0401\n","Epoch 122, Loss: 0.0398\n","Epoch 123, Loss: 0.0398\n","Epoch 124, Loss: 0.0396\n","Epoch 125, Loss: 0.0393\n","Epoch 126, Loss: 0.0392\n","Epoch 127, Loss: 0.0393\n","Epoch 128, Loss: 0.0390\n","Epoch 129, Loss: 0.0389\n","Epoch 130, Loss: 0.0387\n","Epoch 131, Loss: 0.0386\n","Epoch 132, Loss: 0.0386\n","Epoch 133, Loss: 0.0384\n","Epoch 134, Loss: 0.0383\n","Early stopping\n","Model saved as saved_models/model_emb64_ctx5_acttanh.pt\n","Generated Text:\n","picture picture and not a herself that as to might be it however by the spirit of the time in love with her father must be so good an young of a little of what he had been wrong yet he had been less wrong than she had supposedand he had suffered and was very sorryand he was so grateful to mrs. weston and so much in love with miss fairfax and she was so happy herself that there was no being severe and could he have entered the room she must have shaken hands with him as heartily as\n"]}],"source":["embedding_size = embedding_sizes[0]\n","block_size = context_lengths[0]\n","activation_fn = activations[1]\n","print(f\"\\nTraining with Embedding Size: {embedding_size}, Context Length: {block_size}, Activation: {activation_fn.__name__}\")\n","model2 = train_model(embedding_size, block_size, activation_fn)\n","save_model(model2, embedding_size, block_size, activation_fn.__name__)\n","print(\"Generated Text:\")\n","print(generate_text(model2, itos, stoi, block_size, max_length=100))"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1025,"sourceId":1853,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
