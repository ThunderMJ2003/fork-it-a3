{"cells":[{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.540704Z","iopub.status.busy":"2024-10-28T21:04:27.540137Z","iopub.status.idle":"2024-10-28T21:04:27.547038Z","shell.execute_reply":"2024-10-28T21:04:27.545626Z","shell.execute_reply.started":"2024-10-28T21:04:27.540662Z"},"trusted":true},"outputs":[],"source":["#importing libraries\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import urllib.request\n","import re\n","from collections import Counter"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.549439Z","iopub.status.busy":"2024-10-28T21:04:27.548986Z","iopub.status.idle":"2024-10-28T21:04:27.560491Z","shell.execute_reply":"2024-10-28T21:04:27.558650Z","shell.execute_reply.started":"2024-10-28T21:04:27.549391Z"},"trusted":true},"outputs":[],"source":["#setting device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.563208Z","iopub.status.busy":"2024-10-28T21:04:27.562730Z","iopub.status.idle":"2024-10-28T21:04:27.782055Z","shell.execute_reply":"2024-10-28T21:04:27.780850Z","shell.execute_reply.started":"2024-10-28T21:04:27.563162Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{ The Project Gutenberg EBook of Emma, by Jane Austen\\ \\ This eBook is for the use of anyone anywhere at no cost and with\\ almost no restrictions whatsoever. You may copy it, give it away or\\ re-use it under the terms of the Project Gutenberg License included\\ with this eBook or online at www.gutenberg.org\\ \\ \\ Title: Emma\\ \\ Author: Jane Austen\\ \\ Release Date: August, 1994 [Etext #158]\\ Posting Date: January 21, 2010\\ Last Updated: October 17, 2016\\ \\ Language: English\\ \\ Character set encodin\n"]}],"source":["import re\n","\n","#path of the RTF file\n","file_path = \"Emma_by_Jane_Austen.rtf\"\n","\n","#reading the RTF file\n","with open(file_path, 'r', encoding='utf-8') as file:\n","    rtf_content = file.read()\n","\n","#function to clean RTF file\n","def clean_rtf(rtf):\n","    # Remove RTF formatting\n","    # This regex removes everything that isn't plain text\n","    cleaned_text = re.sub(r'{\\\\.*?}', '', rtf)  # Remove RTF groups\n","    cleaned_text = re.sub(r'\\\\[a-z]+\\d* ?', '', cleaned_text)  # Remove RTF commands\n","    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Remove extra spaces\n","    return cleaned_text.strip()\n","\n","#cleaning the RTF text\n","plain_text = clean_rtf(rtf_content)\n","\n","#displaying the first 500 characters of the plain text\n","print(plain_text[:500])\n"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.783782Z","iopub.status.busy":"2024-10-28T21:04:27.783451Z","iopub.status.idle":"2024-10-28T21:04:27.826563Z","shell.execute_reply":"2024-10-28T21:04:27.825301Z","shell.execute_reply.started":"2024-10-28T21:04:27.783747Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" the project gutenberg ebook of emma by jane austen  this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. you may copy it give it away or reuse it under the terms of the project gutenberg license included with this ebook or online at www.gutenberg.org   title emma  author jane austen  release date august 1994 etext 158 posting date january 21 2010 las\n"]}],"source":["#converting text to lowercase\n","plain_text = plain_text.lower()\n","\n","#removing unwanted characters\n","cleaned_text = re.sub('[^a-zA-Z0-9 .]', '', plain_text)\n","\n","#splitting into words\n","words = cleaned_text.split()\n","\n","print(cleaned_text[:400])"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.829482Z","iopub.status.busy":"2024-10-28T21:04:27.829077Z","iopub.status.idle":"2024-10-28T21:04:27.863434Z","shell.execute_reply":"2024-10-28T21:04:27.861947Z","shell.execute_reply.started":"2024-10-28T21:04:27.829437Z"},"trusted":true},"outputs":[],"source":["#creating vocab of unique words\n","words_vocab = sorted(set(words))\n","stoi = {s: i for i, s in enumerate(words_vocab)}\n","itos = {i: s for i, s in enumerate(words_vocab)}"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.865638Z","iopub.status.busy":"2024-10-28T21:04:27.865180Z","iopub.status.idle":"2024-10-28T21:04:27.875165Z","shell.execute_reply":"2024-10-28T21:04:27.873362Z","shell.execute_reply.started":"2024-10-28T21:04:27.865587Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["160442\n"]}],"source":["print(len(words))"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.877058Z","iopub.status.busy":"2024-10-28T21:04:27.876667Z","iopub.status.idle":"2024-10-28T21:04:27.887174Z","shell.execute_reply":"2024-10-28T21:04:27.886052Z","shell.execute_reply.started":"2024-10-28T21:04:27.877020Z"},"trusted":true},"outputs":[],"source":["#function to create input-output pairs\n","def create_dataset(words, block_size):\n","    X, Y = [], []\n","    for i in range(len(words) - block_size):\n","        context = [stoi[words[j]] for j in range(i, i + block_size)]\n","        next_word = stoi[words[i + block_size]]\n","        X.append(context)\n","        Y.append(next_word)\n","    return torch.tensor(X).to(device), torch.tensor(Y).to(device)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.889306Z","iopub.status.busy":"2024-10-28T21:04:27.888734Z","iopub.status.idle":"2024-10-28T21:04:27.903543Z","shell.execute_reply":"2024-10-28T21:04:27.902342Z","shell.execute_reply.started":"2024-10-28T21:04:27.889242Z"},"trusted":true},"outputs":[],"source":["class NextWordMLP(nn.Module):\n","    def __init__(self, block_size, vocab_size, emb_dim, hidden_size, activation):\n","        super().__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_dim)\n","        self.lin1 = nn.Linear(block_size * emb_dim, hidden_size)\n","        self.activation = activation\n","        self.lin2 = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, x):\n","        x = self.emb(x)\n","        x = x.view(x.shape[0], -1)\n","        x = self.activation(self.lin1(x))\n","        x = self.lin2(x)\n","        return x"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.906424Z","iopub.status.busy":"2024-10-28T21:04:27.905880Z","iopub.status.idle":"2024-10-28T21:04:27.915466Z","shell.execute_reply":"2024-10-28T21:04:27.914078Z","shell.execute_reply.started":"2024-10-28T21:04:27.906372Z"},"trusted":true},"outputs":[],"source":["embedding_sizes = [64, 128]\n","context_lengths = [5, 10, 15]\n","activations = [F.relu, torch.tanh]\n","epochs = 500"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.918127Z","iopub.status.busy":"2024-10-28T21:04:27.917595Z","iopub.status.idle":"2024-10-28T21:04:27.930137Z","shell.execute_reply":"2024-10-28T21:04:27.928828Z","shell.execute_reply.started":"2024-10-28T21:04:27.918074Z"},"trusted":true},"outputs":[],"source":["def train_model(embedding_size, block_size, activation_fn):\n","    hidden_size = 512\n","    model = NextWordMLP(block_size, len(stoi), embedding_size, hidden_size, activation_fn).to(device)\n","    loss_fn = nn.CrossEntropyLoss()\n","    opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n","    X, Y = create_dataset(words, block_size)\n","\n","    \n","    batch_size = 512\n","    losses = []\n","    for epoch in range(epochs):\n","        epoch_loss = 0\n","        for i in range(0, len(X), batch_size):\n","            x_batch = X[i:i + batch_size]\n","            y_batch = Y[i:i + batch_size]\n","            y_pred = model(x_batch)\n","            loss = loss_fn(y_pred, y_batch)\n","            loss.backward()\n","            opt.step()\n","            opt.zero_grad()\n","            epoch_loss += loss.item()\n","        \n","        losses.append(epoch_loss / (len(X) // batch_size))\n","        if epoch % 1 == 0:\n","            print(f\"Epoch {epoch}, Loss: {losses[-1]:.4f}\")\n","\n","        #early stopping in case loss plateaus\n","        if len(losses) > 10 and abs(losses[-1] - losses[-10]) < 0.001:\n","            print(\"Early stopping\")\n","            break\n","\n","    return model"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T21:04:27.933415Z","iopub.status.busy":"2024-10-28T21:04:27.932983Z","iopub.status.idle":"2024-10-28T21:04:27.947322Z","shell.execute_reply":"2024-10-28T21:04:27.945947Z","shell.execute_reply.started":"2024-10-28T21:04:27.933373Z"},"trusted":true},"outputs":[],"source":["def generate_text(model, itos, stoi, block_size, max_length=50):\n","    context = [0] * block_size\n","    generated_words = []\n","    for _ in range(max_length):\n","        x = torch.tensor(context).view(1, -1).to(device)\n","        y_pred = model(x)\n","        ix = torch.distributions.categorical.Categorical(logits=y_pred).sample().item()\n","        word = itos[ix]\n","        generated_words.append(word)\n","        context = context[1:] + [ix]\n","    return ' '.join(generated_words)"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["def save_model(model, embedding_size, block_size, activation_fn_name):\n","    model_filename = f\"saved_models/model_emb{embedding_size}_ctx{block_size}_act{activation_fn_name}.pt\"\n","    torch.save(model.state_dict(), model_filename)\n","    print(f\"Model saved as {model_filename}\")"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training with Embedding Size: 64, Context Length: 5, Activation: relu\n","Epoch 0, Loss: 6.5846\n","Epoch 1, Loss: 5.5008\n","Epoch 2, Loss: 4.7264\n","Epoch 3, Loss: 3.7315\n","Epoch 4, Loss: 3.1709\n","Epoch 5, Loss: 2.8659\n","Epoch 6, Loss: 2.6344\n","Epoch 7, Loss: 2.4448\n","Epoch 8, Loss: 2.2835\n","Epoch 9, Loss: 2.1424\n","Epoch 10, Loss: 2.0172\n","Epoch 11, Loss: 1.9042\n","Epoch 12, Loss: 1.8009\n","Epoch 13, Loss: 1.7061\n","Epoch 14, Loss: 1.6177\n","Epoch 15, Loss: 1.5356\n","Epoch 16, Loss: 1.4581\n","Epoch 17, Loss: 1.3852\n","Epoch 18, Loss: 1.3161\n","Epoch 19, Loss: 1.2507\n","Epoch 20, Loss: 1.1882\n","Epoch 21, Loss: 1.1287\n","Epoch 22, Loss: 1.0718\n","Epoch 23, Loss: 1.0170\n","Epoch 24, Loss: 0.9647\n","Epoch 25, Loss: 0.9147\n","Epoch 26, Loss: 0.8663\n","Epoch 27, Loss: 0.8202\n","Epoch 28, Loss: 0.7759\n","Epoch 29, Loss: 0.7332\n","Epoch 30, Loss: 0.6920\n","Epoch 31, Loss: 0.6530\n","Epoch 32, Loss: 0.6153\n","Epoch 33, Loss: 0.5790\n","Epoch 34, Loss: 0.5443\n","Epoch 35, Loss: 0.5114\n","Epoch 36, Loss: 0.4793\n","Epoch 37, Loss: 0.4489\n","Epoch 38, Loss: 0.4198\n","Epoch 39, Loss: 0.3921\n","Epoch 40, Loss: 0.3652\n","Epoch 41, Loss: 0.3402\n","Epoch 42, Loss: 0.3161\n","Epoch 43, Loss: 0.2933\n","Epoch 44, Loss: 0.2718\n","Epoch 45, Loss: 0.2513\n","Epoch 46, Loss: 0.2319\n","Epoch 47, Loss: 0.2145\n","Epoch 48, Loss: 0.1973\n","Epoch 49, Loss: 0.1819\n","Epoch 50, Loss: 0.1681\n","Epoch 51, Loss: 0.1552\n","Epoch 52, Loss: 0.1437\n","Epoch 53, Loss: 0.1338\n","Epoch 54, Loss: 0.1247\n","Epoch 55, Loss: 0.1172\n","Epoch 56, Loss: 0.1109\n","Epoch 57, Loss: 0.1064\n","Epoch 58, Loss: 0.1019\n","Epoch 59, Loss: 0.0983\n","Epoch 60, Loss: 0.0950\n","Epoch 61, Loss: 0.0903\n","Epoch 62, Loss: 0.0868\n","Epoch 63, Loss: 0.0835\n","Epoch 64, Loss: 0.0813\n","Epoch 65, Loss: 0.0793\n","Epoch 66, Loss: 0.0783\n","Epoch 67, Loss: 0.0769\n","Epoch 68, Loss: 0.0759\n","Epoch 69, Loss: 0.0746\n","Epoch 70, Loss: 0.0731\n","Epoch 71, Loss: 0.0719\n","Epoch 72, Loss: 0.0715\n","Epoch 73, Loss: 0.0700\n","Epoch 74, Loss: 0.0694\n","Epoch 75, Loss: 0.0690\n","Epoch 76, Loss: 0.0680\n","Epoch 77, Loss: 0.0672\n","Epoch 78, Loss: 0.0665\n","Epoch 79, Loss: 0.0658\n","Epoch 80, Loss: 0.0652\n","Epoch 81, Loss: 0.0648\n","Epoch 82, Loss: 0.0643\n","Epoch 83, Loss: 0.0634\n","Epoch 84, Loss: 0.0627\n","Epoch 85, Loss: 0.0620\n","Epoch 86, Loss: 0.0610\n","Epoch 87, Loss: 0.0604\n","Epoch 88, Loss: 0.0598\n","Epoch 89, Loss: 0.0592\n","Epoch 90, Loss: 0.0586\n","Epoch 91, Loss: 0.0578\n","Epoch 92, Loss: 0.0572\n","Epoch 93, Loss: 0.0567\n","Epoch 94, Loss: 0.0559\n","Epoch 95, Loss: 0.0553\n","Epoch 96, Loss: 0.0548\n","Epoch 97, Loss: 0.0544\n","Epoch 98, Loss: 0.0533\n","Epoch 99, Loss: 0.0530\n","Epoch 100, Loss: 0.0522\n","Epoch 101, Loss: 0.0518\n","Epoch 102, Loss: 0.0513\n","Epoch 103, Loss: 0.0509\n","Epoch 104, Loss: 0.0501\n","Epoch 105, Loss: 0.0500\n","Epoch 106, Loss: 0.0491\n","Epoch 107, Loss: 0.0491\n","Epoch 108, Loss: 0.0482\n","Epoch 109, Loss: 0.0478\n","Epoch 110, Loss: 0.0470\n","Epoch 111, Loss: 0.0469\n","Epoch 112, Loss: 0.0460\n","Epoch 113, Loss: 0.0458\n","Epoch 114, Loss: 0.0452\n","Epoch 115, Loss: 0.0450\n","Epoch 116, Loss: 0.0443\n","Epoch 117, Loss: 0.0444\n","Epoch 118, Loss: 0.0435\n","Epoch 119, Loss: 0.0433\n","Epoch 120, Loss: 0.0428\n","Epoch 121, Loss: 0.0426\n","Epoch 122, Loss: 0.0419\n","Epoch 123, Loss: 0.0420\n","Epoch 124, Loss: 0.0412\n","Epoch 125, Loss: 0.0410\n","Epoch 126, Loss: 0.0404\n","Epoch 127, Loss: 0.0404\n","Epoch 128, Loss: 0.0401\n","Epoch 129, Loss: 0.0400\n","Epoch 130, Loss: 0.0390\n","Epoch 131, Loss: 0.0388\n","Epoch 132, Loss: 0.0384\n","Epoch 133, Loss: 0.0383\n","Epoch 134, Loss: 0.0376\n","Epoch 135, Loss: 0.0378\n","Epoch 136, Loss: 0.0372\n","Epoch 137, Loss: 0.0370\n","Epoch 138, Loss: 0.0365\n","Epoch 139, Loss: 0.0364\n","Epoch 140, Loss: 0.0357\n","Epoch 141, Loss: 0.0357\n","Epoch 142, Loss: 0.0352\n","Epoch 143, Loss: 0.0353\n","Epoch 144, Loss: 0.0347\n","Epoch 145, Loss: 0.0346\n","Epoch 146, Loss: 0.0341\n","Epoch 147, Loss: 0.0341\n","Epoch 148, Loss: 0.0336\n","Epoch 149, Loss: 0.0335\n","Epoch 150, Loss: 0.0330\n","Epoch 151, Loss: 0.0330\n","Epoch 152, Loss: 0.0325\n","Epoch 153, Loss: 0.0325\n","Epoch 154, Loss: 0.0320\n","Epoch 155, Loss: 0.0319\n","Epoch 156, Loss: 0.0315\n","Epoch 157, Loss: 0.0314\n","Epoch 158, Loss: 0.0310\n","Epoch 159, Loss: 0.0310\n","Epoch 160, Loss: 0.0305\n","Epoch 161, Loss: 0.0304\n","Epoch 162, Loss: 0.0301\n","Epoch 163, Loss: 0.0304\n","Epoch 164, Loss: 0.0527\n","Epoch 165, Loss: 0.1473\n","Epoch 166, Loss: 0.0425\n","Epoch 167, Loss: 0.0232\n","Epoch 168, Loss: 0.0217\n","Epoch 169, Loss: 0.0220\n","Epoch 170, Loss: 0.0225\n","Epoch 171, Loss: 0.0231\n","Epoch 172, Loss: 0.0237\n","Epoch 173, Loss: 0.0244\n","Epoch 174, Loss: 0.0251\n","Epoch 175, Loss: 0.0258\n","Epoch 176, Loss: 0.0265\n","Epoch 177, Loss: 0.0269\n","Epoch 178, Loss: 0.0273\n","Epoch 179, Loss: 0.0277\n","Epoch 180, Loss: 0.0278\n","Epoch 181, Loss: 0.0280\n","Epoch 182, Loss: 0.0283\n","Epoch 183, Loss: 0.0283\n","Epoch 184, Loss: 0.0283\n","Epoch 185, Loss: 0.0282\n","Epoch 186, Loss: 0.0282\n","Epoch 187, Loss: 0.0280\n","Early stopping\n","Model saved as saved_models/model_emb64_ctx5_actrelu.pt\n","Generated Text:\n","to your of miss fairfax92s situation in life. i will move a little farther off.94 93i certainly do forget to think of her94 said emma 93as having ever been any thing but my friend and my dearest friend.94 he looked as if he felt to me me. i am still not the least at randalls. up could want of miss fairfax92s greater words for what the knowing he was though but at his daughter92s being cannot be take it so as to what had been most happy young superior an admiration to all so long a party from her well\n"]}],"source":["embedding_size = embedding_sizes[0]\n","block_size = context_lengths[0]\n","activation_fn = activations[0]\n","print(f\"\\nTraining with Embedding Size: {embedding_size}, Context Length: {block_size}, Activation: {activation_fn.__name__}\")\n","model1 = train_model(embedding_size, block_size, activation_fn)\n","save_model(model1, embedding_size, block_size, activation_fn.__name__)\n","print(\"Generated Text:\")\n","print(generate_text(model1, itos, stoi, block_size, max_length=100))"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training with Embedding Size: 64, Context Length: 5, Activation: tanh\n","Epoch 0, Loss: 6.8163\n","Epoch 1, Loss: 5.5102\n","Epoch 2, Loss: 4.8018\n","Epoch 3, Loss: 4.1668\n","Epoch 4, Loss: 3.7250\n","Epoch 5, Loss: 3.4147\n","Epoch 6, Loss: 3.1718\n","Epoch 7, Loss: 2.9695\n","Epoch 8, Loss: 2.7932\n","Epoch 9, Loss: 2.6370\n","Epoch 10, Loss: 2.4941\n","Epoch 11, Loss: 2.3626\n","Epoch 12, Loss: 2.2395\n","Epoch 13, Loss: 2.1240\n","Epoch 14, Loss: 2.0145\n","Epoch 15, Loss: 1.9110\n","Epoch 16, Loss: 1.8115\n","Epoch 17, Loss: 1.7170\n","Epoch 18, Loss: 1.6260\n","Epoch 19, Loss: 1.5390\n","Epoch 20, Loss: 1.4551\n","Epoch 21, Loss: 1.3752\n","Epoch 22, Loss: 1.2977\n","Epoch 23, Loss: 1.2238\n","Epoch 24, Loss: 1.1525\n","Epoch 25, Loss: 1.0849\n","Epoch 26, Loss: 1.0195\n","Epoch 27, Loss: 0.9575\n","Epoch 28, Loss: 0.8982\n","Epoch 29, Loss: 0.8414\n","Epoch 30, Loss: 0.7873\n","Epoch 31, Loss: 0.7365\n","Epoch 32, Loss: 0.6873\n","Epoch 33, Loss: 0.6410\n","Epoch 34, Loss: 0.5972\n","Epoch 35, Loss: 0.5557\n","Epoch 36, Loss: 0.5164\n","Epoch 37, Loss: 0.4789\n","Epoch 38, Loss: 0.4437\n","Epoch 39, Loss: 0.4105\n","Epoch 40, Loss: 0.3794\n","Epoch 41, Loss: 0.3503\n","Epoch 42, Loss: 0.3226\n","Epoch 43, Loss: 0.2973\n","Epoch 44, Loss: 0.2733\n","Epoch 45, Loss: 0.2512\n","Epoch 46, Loss: 0.2305\n","Epoch 47, Loss: 0.2118\n","Epoch 48, Loss: 0.1942\n","Epoch 49, Loss: 0.1787\n","Epoch 50, Loss: 0.1640\n","Epoch 51, Loss: 0.1515\n","Epoch 52, Loss: 0.1393\n","Epoch 53, Loss: 0.1294\n","Epoch 54, Loss: 0.1200\n","Epoch 55, Loss: 0.1122\n","Epoch 56, Loss: 0.1050\n","Epoch 57, Loss: 0.0996\n","Epoch 58, Loss: 0.0941\n","Epoch 59, Loss: 0.0900\n","Epoch 60, Loss: 0.0860\n","Epoch 61, Loss: 0.0828\n","Epoch 62, Loss: 0.0795\n","Epoch 63, Loss: 0.0773\n","Epoch 64, Loss: 0.0746\n","Epoch 65, Loss: 0.0725\n","Epoch 66, Loss: 0.0704\n","Epoch 67, Loss: 0.0687\n","Epoch 68, Loss: 0.0668\n","Epoch 69, Loss: 0.0658\n","Epoch 70, Loss: 0.0637\n","Epoch 71, Loss: 0.0629\n","Epoch 72, Loss: 0.0618\n","Epoch 73, Loss: 0.0609\n","Epoch 74, Loss: 0.0590\n","Epoch 75, Loss: 0.0587\n","Epoch 76, Loss: 0.0573\n","Epoch 77, Loss: 0.0567\n","Epoch 78, Loss: 0.0555\n","Epoch 79, Loss: 0.0549\n","Epoch 80, Loss: 0.0540\n","Epoch 81, Loss: 0.0538\n","Epoch 82, Loss: 0.0527\n","Epoch 83, Loss: 0.0522\n","Epoch 84, Loss: 0.0519\n","Epoch 85, Loss: 0.0512\n","Epoch 86, Loss: 0.0503\n","Epoch 87, Loss: 0.0498\n","Epoch 88, Loss: 0.0492\n","Epoch 89, Loss: 0.0488\n","Epoch 90, Loss: 0.0485\n","Epoch 91, Loss: 0.0479\n","Epoch 92, Loss: 0.0474\n","Epoch 93, Loss: 0.0475\n","Epoch 94, Loss: 0.0466\n","Epoch 95, Loss: 0.0464\n","Epoch 96, Loss: 0.0463\n","Epoch 97, Loss: 0.0458\n","Epoch 98, Loss: 0.0451\n","Epoch 99, Loss: 0.0453\n","Epoch 100, Loss: 0.0445\n","Epoch 101, Loss: 0.0443\n","Epoch 102, Loss: 0.0442\n","Epoch 103, Loss: 0.0439\n","Epoch 104, Loss: 0.0434\n","Epoch 105, Loss: 0.0433\n","Epoch 106, Loss: 0.0430\n","Epoch 107, Loss: 0.0428\n","Epoch 108, Loss: 0.0426\n","Epoch 109, Loss: 0.0425\n","Epoch 110, Loss: 0.0419\n","Epoch 111, Loss: 0.0421\n","Epoch 112, Loss: 0.0416\n","Epoch 113, Loss: 0.0414\n","Epoch 114, Loss: 0.0413\n","Epoch 115, Loss: 0.0411\n","Epoch 116, Loss: 0.0407\n","Epoch 117, Loss: 0.0409\n","Epoch 118, Loss: 0.0405\n","Epoch 119, Loss: 0.0403\n","Epoch 120, Loss: 0.0402\n","Epoch 121, Loss: 0.0401\n","Epoch 122, Loss: 0.0398\n","Epoch 123, Loss: 0.0398\n","Epoch 124, Loss: 0.0396\n","Epoch 125, Loss: 0.0393\n","Epoch 126, Loss: 0.0392\n","Epoch 127, Loss: 0.0393\n","Epoch 128, Loss: 0.0390\n","Epoch 129, Loss: 0.0389\n","Epoch 130, Loss: 0.0387\n","Epoch 131, Loss: 0.0386\n","Epoch 132, Loss: 0.0386\n","Epoch 133, Loss: 0.0384\n","Epoch 134, Loss: 0.0383\n","Early stopping\n","Model saved as saved_models/model_emb64_ctx5_acttanh.pt\n","Generated Text:\n","picture picture and not a herself that as to might be it however by the spirit of the time in love with her father must be so good an young of a little of what he had been wrong yet he had been less wrong than she had supposedand he had suffered and was very sorryand he was so grateful to mrs. weston and so much in love with miss fairfax and she was so happy herself that there was no being severe and could he have entered the room she must have shaken hands with him as heartily as\n"]}],"source":["embedding_size = embedding_sizes[0]\n","block_size = context_lengths[0]\n","activation_fn = activations[1]\n","print(f\"\\nTraining with Embedding Size: {embedding_size}, Context Length: {block_size}, Activation: {activation_fn.__name__}\")\n","model2 = train_model(embedding_size, block_size, activation_fn)\n","save_model(model2, embedding_size, block_size, activation_fn.__name__)\n","print(\"Generated Text:\")\n","print(generate_text(model2, itos, stoi, block_size, max_length=100))"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training with Embedding Size: 64, Context Length: 10, Activation: relu\n","Epoch 0, Loss: 6.6306\n","Epoch 1, Loss: 5.5228\n","Epoch 2, Loss: 4.6195\n","Epoch 3, Loss: 3.4069\n","Epoch 4, Loss: 2.7717\n","Epoch 5, Loss: 2.4056\n","Epoch 6, Loss: 2.1283\n","Epoch 7, Loss: 1.9029\n","Epoch 8, Loss: 1.7122\n","Epoch 9, Loss: 1.5462\n","Epoch 10, Loss: 1.3982\n","Epoch 11, Loss: 1.2643\n","Epoch 12, Loss: 1.1418\n","Epoch 13, Loss: 1.0285\n","Epoch 14, Loss: 0.9229\n","Epoch 15, Loss: 0.8243\n","Epoch 16, Loss: 0.7318\n","Epoch 17, Loss: 0.6448\n","Epoch 18, Loss: 0.5631\n","Epoch 19, Loss: 0.4870\n","Epoch 20, Loss: 0.4160\n","Epoch 21, Loss: 0.3505\n","Epoch 22, Loss: 0.2909\n","Epoch 23, Loss: 0.2378\n","Epoch 24, Loss: 0.1917\n","Epoch 25, Loss: 0.1539\n","Epoch 26, Loss: 0.1245\n","Epoch 27, Loss: 0.1048\n","Epoch 28, Loss: 0.0916\n","Epoch 29, Loss: 0.0834\n","Epoch 30, Loss: 0.0731\n","Epoch 31, Loss: 0.0605\n","Epoch 32, Loss: 0.0478\n","Epoch 33, Loss: 0.0375\n","Epoch 34, Loss: 0.0294\n","Epoch 35, Loss: 0.0239\n","Epoch 36, Loss: 0.0188\n","Epoch 37, Loss: 0.0172\n","Epoch 38, Loss: 0.0151\n","Epoch 39, Loss: 0.0125\n","Epoch 40, Loss: 0.0088\n","Epoch 41, Loss: 0.0065\n","Epoch 42, Loss: 0.0052\n","Epoch 43, Loss: 0.0047\n","Epoch 44, Loss: 0.0041\n","Epoch 45, Loss: 0.0039\n","Epoch 46, Loss: 0.0036\n","Epoch 47, Loss: 0.0035\n","Epoch 48, Loss: 0.0031\n","Epoch 49, Loss: 0.0030\n","Epoch 50, Loss: 0.0027\n","Epoch 51, Loss: 0.4781\n","Epoch 52, Loss: 0.3529\n","Epoch 53, Loss: 0.0379\n","Epoch 54, Loss: 0.0096\n","Epoch 55, Loss: 0.0062\n","Epoch 56, Loss: 0.0052\n","Epoch 57, Loss: 0.0049\n","Epoch 58, Loss: 0.0052\n","Epoch 59, Loss: 0.0060\n","Epoch 60, Loss: 0.0057\n","Epoch 61, Loss: 0.0038\n","Epoch 62, Loss: 0.0033\n","Epoch 63, Loss: 0.0030\n","Epoch 64, Loss: 0.0028\n","Epoch 65, Loss: 0.0027\n","Epoch 66, Loss: 0.0025\n","Epoch 67, Loss: 0.0024\n","Epoch 68, Loss: 0.0023\n","Epoch 69, Loss: 0.0022\n","Epoch 70, Loss: 0.0020\n","Epoch 71, Loss: 0.0019\n","Epoch 72, Loss: 0.0018\n","Epoch 73, Loss: 0.0018\n","Epoch 74, Loss: 0.0017\n","Epoch 75, Loss: 0.0015\n","Early stopping\n","Model saved as saved_models/model_emb64_ctx10_actrelu.pt\n","Generated Text:\n","but not mind of it as any thing for me to be able i should go in an idea and when i am my children that you will laugh it to me to be that i would never keep my word of perfect mr. knightley he is not that it to be the places she saw him to talk and in a moment when she was now in his father92s marriage 93i am glad with him very little for it the very style of a very few those which i had not persuade him and mrs. elton with them up\n"]}],"source":["embedding_size = embedding_sizes[0]\n","block_size = context_lengths[1]\n","activation_fn = activations[0]\n","print(f\"\\nTraining with Embedding Size: {embedding_size}, Context Length: {block_size}, Activation: {activation_fn.__name__}\")\n","model3 = train_model(embedding_size, block_size, activation_fn)\n","save_model(model3, embedding_size, block_size, activation_fn.__name__)\n","print(\"Generated Text:\")\n","print(generate_text(model3, itos, stoi, block_size, max_length=100))"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training with Embedding Size: 64, Context Length: 10, Activation: tanh\n","Epoch 0, Loss: 6.8201\n","Epoch 1, Loss: 5.4913\n","Epoch 2, Loss: 4.6565\n","Epoch 3, Loss: 3.8966\n","Epoch 4, Loss: 3.3750\n","Epoch 5, Loss: 3.0077\n","Epoch 6, Loss: 2.7192\n","Epoch 7, Loss: 2.4784\n","Epoch 8, Loss: 2.2697\n","Epoch 9, Loss: 2.0840\n","Epoch 10, Loss: 1.9153\n","Epoch 11, Loss: 1.7603\n","Epoch 12, Loss: 1.6160\n","Epoch 13, Loss: 1.4808\n","Epoch 14, Loss: 1.3533\n","Epoch 15, Loss: 1.2327\n","Epoch 16, Loss: 1.1184\n","Epoch 17, Loss: 1.0101\n","Epoch 18, Loss: 0.9075\n","Epoch 19, Loss: 0.8107\n","Epoch 20, Loss: 0.7195\n","Epoch 21, Loss: 0.6340\n","Epoch 22, Loss: 0.5543\n","Epoch 23, Loss: 0.4805\n","Epoch 24, Loss: 0.4126\n","Epoch 25, Loss: 0.3509\n","Epoch 26, Loss: 0.2952\n","Epoch 27, Loss: 0.2457\n","Epoch 28, Loss: 0.2023\n","Epoch 29, Loss: 0.1651\n","Epoch 30, Loss: 0.1337\n","Epoch 31, Loss: 0.1080\n","Epoch 32, Loss: 0.0873\n","Epoch 33, Loss: 0.0711\n","Epoch 34, Loss: 0.0584\n","Epoch 35, Loss: 0.0486\n","Epoch 36, Loss: 0.0407\n","Epoch 37, Loss: 0.0346\n","Epoch 38, Loss: 0.0294\n","Epoch 39, Loss: 0.0252\n","Epoch 40, Loss: 0.0215\n","Epoch 41, Loss: 0.0186\n","Epoch 42, Loss: 0.0160\n","Epoch 43, Loss: 0.0139\n","Epoch 44, Loss: 0.0119\n","Epoch 45, Loss: 0.0104\n","Epoch 46, Loss: 0.0091\n","Epoch 47, Loss: 0.0080\n","Epoch 48, Loss: 0.0068\n","Epoch 49, Loss: 0.0060\n","Epoch 50, Loss: 0.0051\n","Epoch 51, Loss: 0.0046\n","Epoch 52, Loss: 0.0039\n","Epoch 53, Loss: 0.0035\n","Epoch 54, Loss: 0.0030\n","Epoch 55, Loss: 0.0028\n","Epoch 56, Loss: 0.0024\n","Epoch 57, Loss: 0.0022\n","Epoch 58, Loss: 0.0019\n","Epoch 59, Loss: 0.0017\n","Epoch 60, Loss: 0.0015\n","Epoch 61, Loss: 0.3320\n","Epoch 62, Loss: 0.0943\n","Epoch 63, Loss: 0.0091\n","Epoch 64, Loss: 0.0051\n","Epoch 65, Loss: 0.0043\n","Epoch 66, Loss: 0.0038\n","Epoch 67, Loss: 0.0034\n","Epoch 68, Loss: 0.0031\n","Epoch 69, Loss: 0.0028\n","Epoch 70, Loss: 0.0026\n","Epoch 71, Loss: 0.0024\n","Epoch 72, Loss: 0.0023\n","Epoch 73, Loss: 0.0022\n","Epoch 74, Loss: 0.0020\n","Epoch 75, Loss: 0.0019\n","Epoch 76, Loss: 0.0018\n","Epoch 77, Loss: 0.0017\n","Epoch 78, Loss: 0.0016\n","Epoch 79, Loss: 0.0016\n","Epoch 80, Loss: 0.0014\n","Early stopping\n","Model saved as saved_models/model_emb64_ctx10_acttanh.pt\n","Generated Text:\n","has been a hundred young man is the young man just four place ought to be pay but in her and so good spirits was a good deal on the want to mrs. weston she is it on that she had said it was all that she felt that one might keep him too little knightley to be talked as you could wonder with a very indifferent most years of the character of course it seemed as to have one better by her own saying so and something 93that is what join how we really do not know what i\n"]}],"source":["embedding_size = embedding_sizes[0]\n","block_size = context_lengths[1]\n","activation_fn = activations[1]\n","print(f\"\\nTraining with Embedding Size: {embedding_size}, Context Length: {block_size}, Activation: {activation_fn.__name__}\")\n","model4 = train_model(embedding_size, block_size, activation_fn)\n","save_model(model4, embedding_size, block_size, activation_fn.__name__)\n","print(\"Generated Text:\")\n","print(generate_text(model4, itos, stoi, block_size, max_length=100))"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training with Embedding Size: 64, Context Length: 15, Activation: relu\n","Epoch 0, Loss: 6.6680\n","Epoch 1, Loss: 5.5343\n","Epoch 2, Loss: 4.5362\n","Epoch 3, Loss: 3.1946\n","Epoch 4, Loss: 2.4880\n","Epoch 5, Loss: 2.0680\n","Epoch 6, Loss: 1.7493\n","Epoch 7, Loss: 1.4897\n","Epoch 8, Loss: 1.2700\n","Epoch 9, Loss: 1.0789\n","Epoch 10, Loss: 0.9090\n","Epoch 11, Loss: 0.7565\n","Epoch 12, Loss: 0.6181\n","Epoch 13, Loss: 0.4932\n","Epoch 14, Loss: 0.3813\n","Epoch 15, Loss: 0.2841\n","Epoch 16, Loss: 0.2031\n","Epoch 17, Loss: 0.1415\n","Epoch 18, Loss: 0.1006\n","Epoch 19, Loss: 0.0783\n","Epoch 20, Loss: 0.0684\n","Epoch 21, Loss: 0.0673\n","Epoch 22, Loss: 0.0605\n","Epoch 23, Loss: 0.0515\n","Epoch 24, Loss: 0.0449\n","Epoch 25, Loss: 0.0366\n","Epoch 26, Loss: 0.0275\n","Epoch 27, Loss: 0.0182\n","Epoch 28, Loss: 0.0111\n","Epoch 29, Loss: 0.0067\n","Epoch 30, Loss: 0.0049\n","Epoch 31, Loss: 0.0043\n","Epoch 32, Loss: 0.0037\n","Epoch 33, Loss: 0.0036\n","Epoch 34, Loss: 0.0032\n","Epoch 35, Loss: 0.0032\n","Epoch 36, Loss: 0.0029\n","Epoch 37, Loss: 0.0029\n","Epoch 38, Loss: 0.0026\n","Epoch 39, Loss: 0.0025\n","Epoch 40, Loss: 0.0022\n","Epoch 41, Loss: 0.0023\n","Epoch 42, Loss: 0.0021\n","Epoch 43, Loss: 0.0019\n","Epoch 44, Loss: 0.0016\n","Epoch 45, Loss: 0.0017\n","Epoch 46, Loss: 0.6826\n","Epoch 47, Loss: 0.4961\n","Epoch 48, Loss: 0.0394\n","Epoch 49, Loss: 0.0074\n","Epoch 50, Loss: 0.0046\n","Epoch 51, Loss: 0.0038\n","Epoch 52, Loss: 0.0034\n","Epoch 53, Loss: 0.0030\n","Epoch 54, Loss: 0.0028\n","Epoch 55, Loss: 0.0026\n","Epoch 56, Loss: 0.0023\n","Epoch 57, Loss: 0.0022\n","Epoch 58, Loss: 0.0020\n","Epoch 59, Loss: 0.0019\n","Epoch 60, Loss: 0.0017\n","Epoch 61, Loss: 0.0017\n","Epoch 62, Loss: 0.0015\n","Epoch 63, Loss: 0.0015\n","Epoch 64, Loss: 0.0013\n","Epoch 65, Loss: 0.0013\n","Epoch 66, Loss: 0.0011\n","Epoch 67, Loss: 0.0011\n","Early stopping\n","Model saved as saved_models/model_emb64_ctx15_actrelu.pt\n","Generated Text:\n","would be good said emma not say alone 93i hope i have a musical or other but evil is pretty very or sort of all years as to brother and therefore really quite any not you said my aunt a great deal better and i can say be taken air by any thing to meet or in a tone been or to write and or perhaps when they had been in one of the project gutenbergtm or old her. he had i hope appeared she thought a mere daughter and a very extraordinary for him soon and away were very\n"]}],"source":["embedding_size = embedding_sizes[0]\n","block_size = context_lengths[2]\n","activation_fn = activations[0]\n","print(f\"\\nTraining with Embedding Size: {embedding_size}, Context Length: {block_size}, Activation: {activation_fn.__name__}\")\n","model5 = train_model(embedding_size, block_size, activation_fn)\n","save_model(model5, embedding_size, block_size, activation_fn.__name__)\n","print(\"Generated Text:\")\n","print(generate_text(model5, itos, stoi, block_size, max_length=100))"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training with Embedding Size: 64, Context Length: 15, Activation: tanh\n","Epoch 0, Loss: 6.7878\n","Epoch 1, Loss: 5.4677\n","Epoch 2, Loss: 4.5579\n","Epoch 3, Loss: 3.7167\n","Epoch 4, Loss: 3.1245\n","Epoch 5, Loss: 2.7075\n","Epoch 6, Loss: 2.3803\n","Epoch 7, Loss: 2.1087\n","Epoch 8, Loss: 1.8745\n","Epoch 9, Loss: 1.6674\n","Epoch 10, Loss: 1.4804\n","Epoch 11, Loss: 1.3090\n","Epoch 12, Loss: 1.1501\n","Epoch 13, Loss: 1.0022\n","Epoch 14, Loss: 0.8643\n","Epoch 15, Loss: 0.7363\n","Epoch 16, Loss: 0.6183\n","Epoch 17, Loss: 0.5107\n","Epoch 18, Loss: 0.4142\n","Epoch 19, Loss: 0.3292\n","Epoch 20, Loss: 0.2563\n","Epoch 21, Loss: 0.1958\n","Epoch 22, Loss: 0.1478\n","Epoch 23, Loss: 0.1114\n","Epoch 24, Loss: 0.0851\n","Epoch 25, Loss: 0.0665\n","Epoch 26, Loss: 0.0532\n","Epoch 27, Loss: 0.0433\n","Epoch 28, Loss: 0.0358\n","Epoch 29, Loss: 0.0299\n","Epoch 30, Loss: 0.0250\n","Epoch 31, Loss: 0.0211\n","Epoch 32, Loss: 0.0179\n","Epoch 33, Loss: 0.0152\n","Epoch 34, Loss: 0.0129\n","Epoch 35, Loss: 0.0111\n","Epoch 36, Loss: 0.0095\n","Epoch 37, Loss: 0.0082\n","Epoch 38, Loss: 0.0070\n","Epoch 39, Loss: 0.0060\n","Epoch 40, Loss: 0.0052\n","Epoch 41, Loss: 0.0045\n","Epoch 42, Loss: 0.0039\n","Epoch 43, Loss: 0.0034\n","Epoch 44, Loss: 0.0029\n","Epoch 45, Loss: 0.0025\n","Epoch 46, Loss: 0.0022\n","Epoch 47, Loss: 0.0019\n","Epoch 48, Loss: 0.0017\n","Epoch 49, Loss: 0.0015\n","Epoch 50, Loss: 0.0013\n","Epoch 51, Loss: 0.0012\n","Epoch 52, Loss: 0.0010\n","Epoch 53, Loss: 0.0009\n","Epoch 54, Loss: 0.0008\n","Epoch 55, Loss: 0.0007\n","Epoch 56, Loss: 0.0006\n","Epoch 57, Loss: 0.0006\n","Epoch 58, Loss: 0.0005\n","Early stopping\n","Model saved as saved_models/model_emb64_ctx15_acttanh.pt\n","Generated Text:\n","she she she she will not she she and she she then she she she gone herself she was not herself she was too much of her being a most amiable mr. emma at hartfield had were in her with a friend upon his goodwill or any longer in him 93we have been sorry for harriet with you.94 we said harriet only never have heard with her but for you had wanted it was now only only from your father92s and it must be farther and miss bates who had been at a look must be on for hartfield and\n"]}],"source":["embedding_size = embedding_sizes[0]\n","block_size = context_lengths[2]\n","activation_fn = activations[1]\n","print(f\"\\nTraining with Embedding Size: {embedding_size}, Context Length: {block_size}, Activation: {activation_fn.__name__}\")\n","model6 = train_model(embedding_size, block_size, activation_fn)\n","save_model(model6, embedding_size, block_size, activation_fn.__name__)\n","print(\"Generated Text:\")\n","print(generate_text(model6, itos, stoi, block_size, max_length=100))"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training with Embedding Size: 128, Context Length: 5, Activation: relu\n","Epoch 0, Loss: 6.4765\n","Epoch 1, Loss: 5.3467\n","Epoch 2, Loss: 4.5104\n","Epoch 3, Loss: 3.4308\n","Epoch 4, Loss: 2.7926\n","Epoch 5, Loss: 2.4139\n","Epoch 6, Loss: 2.1232\n","Epoch 7, Loss: 1.8838\n","Epoch 8, Loss: 1.6787\n","Epoch 9, Loss: 1.4991\n","Epoch 10, Loss: 1.3388\n","Epoch 11, Loss: 1.1951\n","Epoch 12, Loss: 1.0638\n","Epoch 13, Loss: 0.9449\n","Epoch 14, Loss: 0.8353\n","Epoch 15, Loss: 0.7353\n","Epoch 16, Loss: 0.6432\n","Epoch 17, Loss: 0.5604\n","Epoch 18, Loss: 0.4838\n","Epoch 19, Loss: 0.4157\n","Epoch 20, Loss: 0.3540\n","Epoch 21, Loss: 0.2999\n","Epoch 22, Loss: 0.2525\n","Epoch 23, Loss: 0.2124\n","Epoch 24, Loss: 0.1788\n","Epoch 25, Loss: 0.1526\n","Epoch 26, Loss: 0.1324\n","Epoch 27, Loss: 0.1181\n","Epoch 28, Loss: 0.1073\n","Epoch 29, Loss: 0.1009\n","Epoch 30, Loss: 0.0959\n","Epoch 31, Loss: 0.0926\n","Epoch 32, Loss: 0.0892\n","Epoch 33, Loss: 0.0865\n","Epoch 34, Loss: 0.0839\n","Epoch 35, Loss: 0.0819\n","Epoch 36, Loss: 0.0801\n","Epoch 37, Loss: 0.0788\n","Epoch 38, Loss: 0.0777\n","Epoch 39, Loss: 0.0765\n","Epoch 40, Loss: 0.0756\n","Epoch 41, Loss: 0.0751\n","Epoch 42, Loss: 0.0743\n","Epoch 43, Loss: 0.0740\n","Epoch 44, Loss: 0.0724\n","Epoch 45, Loss: 0.0698\n","Epoch 46, Loss: 0.0679\n","Epoch 47, Loss: 0.0659\n","Epoch 48, Loss: 0.0649\n","Epoch 49, Loss: 0.0632\n","Epoch 50, Loss: 0.0621\n","Epoch 51, Loss: 0.0604\n","Epoch 52, Loss: 0.0595\n","Epoch 53, Loss: 0.0575\n","Epoch 54, Loss: 0.0567\n","Epoch 55, Loss: 0.0550\n","Epoch 56, Loss: 0.0541\n","Epoch 57, Loss: 0.0526\n","Epoch 58, Loss: 0.0517\n","Epoch 59, Loss: 0.0508\n","Epoch 60, Loss: 0.0500\n","Epoch 61, Loss: 0.0484\n","Epoch 62, Loss: 0.0491\n","Epoch 63, Loss: 0.0751\n","Epoch 64, Loss: 0.1801\n","Epoch 65, Loss: 0.0666\n","Epoch 66, Loss: 0.0349\n","Epoch 67, Loss: 0.0322\n","Epoch 68, Loss: 0.0327\n","Epoch 69, Loss: 0.0333\n","Epoch 70, Loss: 0.0345\n","Epoch 71, Loss: 0.0353\n","Epoch 72, Loss: 0.0364\n","Epoch 73, Loss: 0.0371\n","Epoch 74, Loss: 0.0378\n","Epoch 75, Loss: 0.0381\n","Epoch 76, Loss: 0.0388\n","Epoch 77, Loss: 0.0383\n","Epoch 78, Loss: 0.0385\n","Epoch 79, Loss: 0.0380\n","Epoch 80, Loss: 0.0382\n","Epoch 81, Loss: 0.0375\n","Epoch 82, Loss: 0.0372\n","Early stopping\n","Model saved as saved_models/model_emb128_ctx5_actrelu.pt\n","Generated Text:\n","myself by making her secret but she must have been no perpetual enemy. i am glad to side of emma92s situation the right was made ill but i must imagine that i own emma was as strong as it may be perfectly only that if you will become earlier opened you see never never heard at least idea of the body is not likely to be less of the man it seemed a peculiarly cruel necessity that was to be placing her in such a state of spirits never in any thing like it and it was with difficulty that\n"]}],"source":["embedding_size = embedding_sizes[1]\n","block_size = context_lengths[0]\n","activation_fn = activations[0]\n","print(f\"\\nTraining with Embedding Size: {embedding_size}, Context Length: {block_size}, Activation: {activation_fn.__name__}\")\n","model7 = train_model(embedding_size, block_size, activation_fn)\n","save_model(model7, embedding_size, block_size, activation_fn.__name__)\n","print(\"Generated Text:\")\n","print(generate_text(model7, itos, stoi, block_size, max_length=100))"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training with Embedding Size: 128, Context Length: 5, Activation: tanh\n","Epoch 0, Loss: 6.6657\n","Epoch 1, Loss: 5.3424\n","Epoch 2, Loss: 4.5681\n","Epoch 3, Loss: 3.8796\n","Epoch 4, Loss: 3.3790\n","Epoch 5, Loss: 3.0083\n","Epoch 6, Loss: 2.7072\n","Epoch 7, Loss: 2.4508\n","Epoch 8, Loss: 2.2238\n","Epoch 9, Loss: 2.0203\n","Epoch 10, Loss: 1.8338\n","Epoch 11, Loss: 1.6627\n","Epoch 12, Loss: 1.5035\n","Epoch 13, Loss: 1.3553\n","Epoch 14, Loss: 1.2163\n","Epoch 15, Loss: 1.0872\n","Epoch 16, Loss: 0.9668\n","Epoch 17, Loss: 0.8558\n","Epoch 18, Loss: 0.7534\n","Epoch 19, Loss: 0.6596\n","Epoch 20, Loss: 0.5743\n","Epoch 21, Loss: 0.4971\n","Epoch 22, Loss: 0.4281\n","Epoch 23, Loss: 0.3663\n","Epoch 24, Loss: 0.3122\n","Epoch 25, Loss: 0.2654\n","Epoch 26, Loss: 0.2252\n","Epoch 27, Loss: 0.1910\n","Epoch 28, Loss: 0.1630\n","Epoch 29, Loss: 0.1400\n","Epoch 30, Loss: 0.1224\n","Epoch 31, Loss: 0.1085\n","Epoch 32, Loss: 0.0982\n","Epoch 33, Loss: 0.0894\n","Epoch 34, Loss: 0.0831\n","Epoch 35, Loss: 0.0775\n","Epoch 36, Loss: 0.0733\n","Epoch 37, Loss: 0.0698\n","Epoch 38, Loss: 0.0668\n","Epoch 39, Loss: 0.0636\n","Epoch 40, Loss: 0.0615\n","Epoch 41, Loss: 0.0591\n","Epoch 42, Loss: 0.0572\n","Epoch 43, Loss: 0.0554\n","Epoch 44, Loss: 0.0538\n","Epoch 45, Loss: 0.0521\n","Epoch 46, Loss: 0.0512\n","Epoch 47, Loss: 0.0500\n","Epoch 48, Loss: 0.0493\n","Epoch 49, Loss: 0.0482\n","Epoch 50, Loss: 0.0466\n","Epoch 51, Loss: 0.0454\n","Epoch 52, Loss: 0.0446\n","Epoch 53, Loss: 0.0435\n","Epoch 54, Loss: 0.0429\n","Epoch 55, Loss: 0.0421\n","Epoch 56, Loss: 0.0417\n","Epoch 57, Loss: 0.0408\n","Epoch 58, Loss: 0.0407\n","Epoch 59, Loss: 0.0400\n","Epoch 60, Loss: 0.0393\n","Epoch 61, Loss: 0.0391\n","Epoch 62, Loss: 0.0387\n","Epoch 63, Loss: 0.0387\n","Epoch 64, Loss: 0.0393\n","Epoch 65, Loss: 0.0406\n","Epoch 66, Loss: 0.0766\n","Epoch 67, Loss: 0.0952\n","Epoch 68, Loss: 0.0407\n","Early stopping\n","Model saved as saved_models/model_emb128_ctx5_acttanh.pt\n","Generated Text:\n","have a better curiosity of him. you have without great the certain of miss woodhouse92s upon the old abbey was as to quarrel with her. she knew that at times she must be missed and could not think without pain of emma92s losing a single pleasure or suffering an hour92s ennui from the want of her companionableness but dear emma was of no feeble character she was more equal to her situation than most girls would have been and had sense and energy and spirits that might be hoped would bear her well and happily through its little difficulties and\n"]}],"source":["embedding_size = embedding_sizes[1]\n","block_size = context_lengths[0]\n","activation_fn = activations[1]\n","print(f\"\\nTraining with Embedding Size: {embedding_size}, Context Length: {block_size}, Activation: {activation_fn.__name__}\")\n","model8 = train_model(embedding_size, block_size, activation_fn)\n","save_model(model8, embedding_size, block_size, activation_fn.__name__)\n","print(\"Generated Text:\")\n","print(generate_text(model8, itos, stoi, block_size, max_length=100))"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training with Embedding Size: 128, Context Length: 10, Activation: relu\n","Epoch 0, Loss: 6.5455\n","Epoch 1, Loss: 5.3559\n","Epoch 2, Loss: 4.3474\n","Epoch 3, Loss: 3.0230\n","Epoch 4, Loss: 2.2512\n","Epoch 5, Loss: 1.7703\n","Epoch 6, Loss: 1.4031\n","Epoch 7, Loss: 1.1030\n","Epoch 8, Loss: 0.8510\n","Epoch 9, Loss: 0.6374\n","Epoch 10, Loss: 0.4580\n","Epoch 11, Loss: 0.3118\n","Epoch 12, Loss: 0.2002\n","Epoch 13, Loss: 0.1245\n","Epoch 14, Loss: 0.0799\n","Epoch 15, Loss: 0.0545\n","Epoch 16, Loss: 0.0369\n","Epoch 17, Loss: 0.0249\n","Epoch 18, Loss: 0.0181\n","Epoch 19, Loss: 0.0140\n","Epoch 20, Loss: 0.0110\n","Epoch 21, Loss: 0.0093\n","Epoch 22, Loss: 0.0079\n","Epoch 23, Loss: 0.0070\n","Epoch 24, Loss: 0.0059\n","Epoch 25, Loss: 0.0053\n","Epoch 26, Loss: 0.0046\n","Epoch 27, Loss: 0.0042\n","Epoch 28, Loss: 0.0036\n","Epoch 29, Loss: 0.0032\n","Epoch 30, Loss: 0.0027\n","Epoch 31, Loss: 0.0025\n","Epoch 32, Loss: 0.0022\n","Epoch 33, Loss: 0.0021\n","Epoch 34, Loss: 0.8769\n","Epoch 35, Loss: 0.3743\n","Epoch 36, Loss: 0.0290\n","Epoch 37, Loss: 0.0070\n","Epoch 38, Loss: 0.0048\n","Epoch 39, Loss: 0.0041\n","Epoch 40, Loss: 0.0036\n","Epoch 41, Loss: 0.0032\n","Epoch 42, Loss: 0.0029\n","Early stopping\n","Model saved as saved_models/model_emb128_ctx10_actrelu.pt\n","Generated Text:\n","he come on replied and said he had not that it was a degree of consequence for harriet smith but she was very clear and the hope that he was often known in some than he had acknowledged her in good feelings and miss woodhouse could almost absolutely her into a little have she had no better this than to be volunteers but i do not say what she was telling christmas it.94 emma wished she had now the evil of no and therefore she could not help history of her having an little in and for a good deal\n"]}],"source":["embedding_size = embedding_sizes[1]\n","block_size = context_lengths[1]\n","activation_fn = activations[0]\n","print(f\"\\nTraining with Embedding Size: {embedding_size}, Context Length: {block_size}, Activation: {activation_fn.__name__}\")\n","model9 = train_model(embedding_size, block_size, activation_fn)\n","save_model(model9, embedding_size, block_size, activation_fn.__name__)\n","print(\"Generated Text:\")\n","print(generate_text(model9, itos, stoi, block_size, max_length=100))"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training with Embedding Size: 128, Context Length: 10, Activation: tanh\n","Epoch 0, Loss: 6.6169\n","Epoch 1, Loss: 5.2919\n","Epoch 2, Loss: 4.3724\n","Epoch 3, Loss: 3.5306\n","Epoch 4, Loss: 2.9099\n","Epoch 5, Loss: 2.4495\n","Epoch 6, Loss: 2.0766\n","Epoch 7, Loss: 1.7620\n","Epoch 8, Loss: 1.4895\n","Epoch 9, Loss: 1.2493\n","Epoch 10, Loss: 1.0350\n","Epoch 11, Loss: 0.8440\n","Epoch 12, Loss: 0.6739\n","Epoch 13, Loss: 0.5249\n","Epoch 14, Loss: 0.3973\n","Epoch 15, Loss: 0.2921\n","Epoch 16, Loss: 0.2089\n","Epoch 17, Loss: 0.1470\n","Epoch 18, Loss: 0.1037\n","Epoch 19, Loss: 0.0753\n","Epoch 20, Loss: 0.0565\n","Epoch 21, Loss: 0.0441\n","Epoch 22, Loss: 0.0351\n","Epoch 23, Loss: 0.0287\n","Epoch 24, Loss: 0.0235\n","Epoch 25, Loss: 0.0197\n","Epoch 26, Loss: 0.0163\n","Epoch 27, Loss: 0.0139\n","Epoch 28, Loss: 0.0116\n","Epoch 29, Loss: 0.0100\n","Epoch 30, Loss: 0.0084\n","Epoch 31, Loss: 0.0073\n","Epoch 32, Loss: 0.0061\n","Epoch 33, Loss: 0.0054\n","Epoch 34, Loss: 0.0045\n","Epoch 35, Loss: 0.0040\n","Epoch 36, Loss: 0.0033\n","Epoch 37, Loss: 0.0030\n","Epoch 38, Loss: 0.0025\n","Epoch 39, Loss: 0.0023\n","Epoch 40, Loss: 0.0019\n","Epoch 41, Loss: 0.0018\n","Epoch 42, Loss: 0.0015\n","Epoch 43, Loss: 0.0015\n","Epoch 44, Loss: 0.0012\n","Epoch 45, Loss: 0.0012\n","Epoch 46, Loss: 0.0009\n","Epoch 47, Loss: 0.0010\n","Epoch 48, Loss: 0.0007\n","Epoch 49, Loss: 0.0130\n","Epoch 50, Loss: 0.6986\n","Epoch 51, Loss: 0.0438\n","Epoch 52, Loss: 0.0069\n","Epoch 53, Loss: 0.0050\n","Epoch 54, Loss: 0.0041\n","Epoch 55, Loss: 0.0036\n","Epoch 56, Loss: 0.0031\n","Epoch 57, Loss: 0.0029\n","Epoch 58, Loss: 0.0025\n","Epoch 59, Loss: 0.0024\n","Epoch 60, Loss: 0.0021\n","Epoch 61, Loss: 0.0020\n","Epoch 62, Loss: 0.0018\n","Epoch 63, Loss: 0.0017\n","Epoch 64, Loss: 0.0015\n","Epoch 65, Loss: 0.0015\n","Epoch 66, Loss: 0.0013\n","Epoch 67, Loss: 0.0013\n","Epoch 68, Loss: 0.0011\n","Epoch 69, Loss: 0.0012\n","Early stopping\n","Model saved as saved_models/model_emb128_ctx10_acttanh.pt\n","Generated Text:\n","he could enjoy syllable he sat smith however in his manner in harriet92s favour of she became of the words and especially of the day before we could not have staid the arm of him. they must not own a family by wine but i had never heard him at a and so much as her as emma as her to be miss woodhouse94 was in her claims and telling she could command the next house of the greatest happiness she will be impossible to be the means in their conversation and his own powers and i have no doubt\n"]}],"source":["embedding_size = embedding_sizes[1]\n","block_size = context_lengths[1]\n","activation_fn = activations[1]\n","print(f\"\\nTraining with Embedding Size: {embedding_size}, Context Length: {block_size}, Activation: {activation_fn.__name__}\")\n","model10 = train_model(embedding_size, block_size, activation_fn)\n","save_model(model10, embedding_size, block_size, activation_fn.__name__)\n","print(\"Generated Text:\")\n","print(generate_text(model10, itos, stoi, block_size, max_length=100))"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training with Embedding Size: 128, Context Length: 15, Activation: relu\n","Epoch 0, Loss: 6.5768\n","Epoch 1, Loss: 5.3418\n","Epoch 2, Loss: 4.2254\n","Epoch 3, Loss: 2.7315\n","Epoch 4, Loss: 1.8117\n","Epoch 5, Loss: 1.2447\n","Epoch 6, Loss: 0.8268\n","Epoch 7, Loss: 0.5083\n","Epoch 8, Loss: 0.2767\n","Epoch 9, Loss: 0.1350\n","Epoch 10, Loss: 0.0691\n","Epoch 11, Loss: 0.0401\n","Epoch 12, Loss: 0.0241\n","Epoch 13, Loss: 0.0166\n","Epoch 14, Loss: 0.0126\n","Epoch 15, Loss: 0.0104\n","Epoch 16, Loss: 0.0086\n","Epoch 17, Loss: 0.0074\n","Epoch 18, Loss: 0.0062\n","Epoch 19, Loss: 0.0055\n","Epoch 20, Loss: 0.0047\n","Epoch 21, Loss: 0.0041\n","Epoch 22, Loss: 0.0035\n","Epoch 23, Loss: 0.0031\n","Epoch 24, Loss: 0.0026\n","Epoch 25, Loss: 0.0023\n","Epoch 26, Loss: 0.0020\n","Epoch 27, Loss: 0.0017\n","Epoch 28, Loss: 0.0016\n","Epoch 29, Loss: 0.0015\n","Epoch 30, Loss: 0.0013\n","Epoch 31, Loss: 0.0011\n","Epoch 32, Loss: 0.0009\n","Epoch 33, Loss: 0.0008\n","Epoch 34, Loss: 0.0008\n","Epoch 35, Loss: 0.0007\n","Epoch 36, Loss: 1.1488\n","Epoch 37, Loss: 0.4880\n","Epoch 38, Loss: 0.0268\n","Epoch 39, Loss: 0.0053\n","Epoch 40, Loss: 0.0037\n","Epoch 41, Loss: 0.0031\n","Epoch 42, Loss: 0.0027\n","Epoch 43, Loss: 0.0024\n","Epoch 44, Loss: 0.0021\n","Epoch 45, Loss: 0.0019\n","Epoch 46, Loss: 0.0017\n","Epoch 47, Loss: 0.0016\n","Epoch 48, Loss: 0.0015\n","Epoch 49, Loss: 0.0013\n","Epoch 50, Loss: 0.0012\n","Epoch 51, Loss: 0.0011\n","Epoch 52, Loss: 0.0010\n","Epoch 53, Loss: 0.0009\n","Epoch 54, Loss: 0.0009\n","Epoch 55, Loss: 0.0007\n","Epoch 56, Loss: 0.0007\n","Early stopping\n","Model saved as saved_models/model_emb128_ctx15_actrelu.pt\n","Generated Text:\n","her better have not a wife a little thing to send from her own heart in a good deal and on which had been the natural morning to be the whole eating her going in home by feeling at hartfield miss bates and she is very first to harriet with robert an second situations i have her own interest in spite than she has seen i think it does not let me often i am sure she did not think i have a understand in love than certainly ever wanted to see how much more us all that had ever\n"]}],"source":["embedding_size = embedding_sizes[1]\n","block_size = context_lengths[2]\n","activation_fn = activations[0]\n","print(f\"\\nTraining with Embedding Size: {embedding_size}, Context Length: {block_size}, Activation: {activation_fn.__name__}\")\n","model11 = train_model(embedding_size, block_size, activation_fn)\n","save_model(model11, embedding_size, block_size, activation_fn.__name__)\n","print(\"Generated Text:\")\n","print(generate_text(model11, itos, stoi, block_size, max_length=100))"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training with Embedding Size: 128, Context Length: 15, Activation: tanh\n","Epoch 0, Loss: 6.6133\n","Epoch 1, Loss: 5.2001\n","Epoch 2, Loss: 4.1508\n","Epoch 3, Loss: 3.1974\n","Epoch 4, Loss: 2.4973\n","Epoch 5, Loss: 1.9754\n","Epoch 6, Loss: 1.5585\n","Epoch 7, Loss: 1.2152\n","Epoch 8, Loss: 0.9265\n","Epoch 9, Loss: 0.6834\n","Epoch 10, Loss: 0.4820\n","Epoch 11, Loss: 0.3228\n","Epoch 12, Loss: 0.2058\n","Epoch 13, Loss: 0.1293\n","Epoch 14, Loss: 0.0843\n","Epoch 15, Loss: 0.0588\n","Epoch 16, Loss: 0.0435\n","Epoch 17, Loss: 0.0336\n","Epoch 18, Loss: 0.0267\n","Epoch 19, Loss: 0.0218\n","Epoch 20, Loss: 0.0178\n","Epoch 21, Loss: 0.0149\n","Epoch 22, Loss: 0.0124\n","Epoch 23, Loss: 0.0105\n","Epoch 24, Loss: 0.0088\n","Epoch 25, Loss: 0.0076\n","Epoch 26, Loss: 0.0064\n","Epoch 27, Loss: 0.0055\n","Epoch 28, Loss: 0.0046\n","Epoch 29, Loss: 0.0040\n","Epoch 30, Loss: 0.0034\n","Epoch 31, Loss: 0.0030\n","Epoch 32, Loss: 0.0025\n","Epoch 33, Loss: 0.0022\n","Epoch 34, Loss: 0.0019\n","Epoch 35, Loss: 0.0017\n","Epoch 36, Loss: 0.0014\n","Epoch 37, Loss: 0.0013\n","Epoch 38, Loss: 0.0011\n","Epoch 39, Loss: 0.0010\n","Epoch 40, Loss: 0.0008\n","Epoch 41, Loss: 0.0008\n","Epoch 42, Loss: 0.0006\n","Epoch 43, Loss: 0.0006\n","Epoch 44, Loss: 0.9879\n","Epoch 45, Loss: 0.2576\n","Epoch 46, Loss: 0.0156\n","Epoch 47, Loss: 0.0075\n","Epoch 48, Loss: 0.0060\n","Epoch 49, Loss: 0.0051\n","Epoch 50, Loss: 0.0044\n","Epoch 51, Loss: 0.0039\n","Epoch 52, Loss: 0.0034\n","Epoch 53, Loss: 0.0031\n","Epoch 54, Loss: 0.0027\n","Epoch 55, Loss: 0.0025\n","Epoch 56, Loss: 0.0022\n","Epoch 57, Loss: 0.0021\n","Epoch 58, Loss: 0.0018\n","Epoch 59, Loss: 0.0017\n","Epoch 60, Loss: 0.0015\n","Epoch 61, Loss: 0.0014\n","Epoch 62, Loss: 0.0012\n","Epoch 63, Loss: 0.0012\n","Epoch 64, Loss: 0.0010\n","Epoch 65, Loss: 0.0010\n","Epoch 66, Loss: 0.0008\n","Epoch 67, Loss: 0.0008\n","Early stopping\n","Model saved as saved_models/model_emb128_ctx15_acttanh.pt\n","Generated Text:\n","friend92s else me.94 better than an hour made mr. and ashamed of it. frank churchill placed quite my regard at him he had his name and by his easy and was very enjoyment to say a wife on them. with a greater his spirits when appeared miss woman chapter were now very feeling and its being any thing before miss bates had already from any thing that their companion were very wrong and been too much for a word of the happiness of a great lady of a girl of it must be a very much indeed they would have\n"]}],"source":["embedding_size = embedding_sizes[1]\n","block_size = context_lengths[2]\n","activation_fn = activations[1]\n","print(f\"\\nTraining with Embedding Size: {embedding_size}, Context Length: {block_size}, Activation: {activation_fn.__name__}\")\n","model12 = train_model(embedding_size, block_size, activation_fn)\n","save_model(model12, embedding_size, block_size, activation_fn.__name__)\n","print(\"Generated Text:\")\n","print(generate_text(model12, itos, stoi, block_size, max_length=100))"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1025,"sourceId":1853,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
